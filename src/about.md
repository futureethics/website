---
title: About
layout: base
---

<div class="page-header">
  <h1>About Future Ethics</h1>
  <p class="page-subtitle">
    We're the team that finds AI safety problems before they become headlines
  </p>
</div>

## Who We Are

Future Ethics is the world's leading multicultural AI safety research organization. We specialize in discovering hidden vulnerabilities that emerge when AI systems encounter languages, cultures, and contexts they weren't designed for.

While most AI safety testing focuses on English-speaking, Western contexts, we've built the expertise and networks to evaluate AI safety across the full spectrum of human cultures and languages. Our work prevents costly failures, regulatory violations, and reputational damage for organizations deploying AI at global scale.

**What makes us unique:**

<div class="two-column">
  <div class="research-area">
    <h4>Global Expertise Network</h4>
    <p>We coordinate 350+ expert red teamers across Asia-Pacific, representing the world's largest multicultural AI safety evaluation network.</p>
  </div>
  <div class="research-area">
    <h4>Proven Methodologies</h4>
    <p>Our research frameworks have been published at top-tier conferences and adopted by major AI labs for cross-cultural safety evaluation.</p>
  </div>
</div>

<div class="two-column">
  <div class="research-area">
    <h4>Real-World Impact</h4>
    <p>We've disclosed 12+ critical vulnerabilities to major AI vendors and helped organizations prevent failures affecting millions of users.</p>
  </div>
  <div class="research-area">
    <h4>Policy Influence</h4>
    <p>Our research directly shapes AI safety policy for governments and international organizations across multiple continents.</p>
  </div>
</div>

**Our approach** combines rigorous academic research with practical industry application. We don't just identify problems‚Äîwe develop actionable solutions that organizations can implement immediately to make their AI systems safer.

<hr class="divider">

## Our Team

<div class="team-grid">
  
  <div class="team-member">
    <div class="team-member-header">
      <h3 class="team-member-name">Adrianna Tan</h3>
      <p class="team-member-title">Founder & Lead Researcher</p>
      <div class="tag-list">
        <span class="tag">AI Safety Research</span>
        <span class="tag">Multicultural Red Teaming</span>
        <span class="tag">Technical Program Management</span>
      </div>
    </div>
    
    <div class="team-member-bio">
      <p>Adrianna builds things that matter. As a technical program manager and AI safety researcher, she's spent the last few years figuring out how to make AI systems less likely to cause harm.</p>
      
      <p>She led the world's first multicultural AI red teaming challenge across Asia-Pacific‚Äîcoordinating 350+ experts across 9 countries and 8 languages to create new methodologies for testing AI safety across cultures, together with Humane Intelligence and Singapore's IMDA.</p>
      
      <p>She's also run AI assurance programs for the U.S. Department of Defense and led safety evaluations for major EdTech platforms. Before diving into AI safety, Adrianna spent over a decade building products that actually work for people‚Äîfrom Indonesia's early mobile payment networks to San Francisco's COVID-19 response systems.</p>
    </div>
    
    <div class="team-member-achievements">
      <h4>Key Achievements</h4>
      <ul>
        <li><strong>ICLR 2025:</strong> Published multicultural AI red teaming research (Oral Presentation)</li>
        <li><strong>DoD Essential Pathfinder:</strong> Designated for AI safety research contributions</li>
        <li><strong>350+ Expert Network:</strong> Built and coordinates largest multicultural AI red team globally</li>
        <li><strong>3,000+ Students:</strong> Founded education nonprofit providing STEM scholarships in Mumbai</li>
      </ul>
    </div>
    
    <div class="team-member-service">
      <h4>Service & Advisory Roles</h4>
      <ul>
        <li>GSA Federal Advisory Committee member</li>
        <li>California AI Policy volunteer, Center for AI & Digital Policy</li>
        <li>Singapore IMDA AI governance framework contributor</li>
        <li>ASEAN AI safety standards contributing expert</li>
      </ul>
    </div>
  </div>

  <!-- Template for future team members -->
  <!--
  <div class="team-member">
    <div class="team-member-header">
      <h3 class="team-member-name">[Name]</h3>
      <p class="team-member-title">[Title]</p>
      <div class="tag-list">
        <span class="tag">[Expertise 1]</span>
        <span class="tag">[Expertise 2]</span>
        <span class="tag">[Expertise 3]</span>
      </div>
    </div>
    
    <div class="team-member-bio">
      <p>[Bio paragraph 1]</p>
      <p>[Bio paragraph 2]</p>
    </div>
    
    <div class="team-member-achievements">
      <h4>Key Achievements</h4>
      <ul>
        <li>[Achievement 1]</li>
        <li>[Achievement 2]</li>
      </ul>
    </div>
  </div>
  -->

</div>

<div class="callout-box">
  <h3>Join Our Mission</h3>
  <p>We're building the future of AI safety across cultures and languages. If you're passionate about making AI systems safer for everyone, everywhere, we'd love to hear from you.</p>
  <a href="/contact" class="cta-button cta-secondary">Get in touch</a>
</div>

<hr class="divider">

## Our Values

<div class="two-column">
  <div class="research-area">
    <h4>üåç Global Perspective</h4>
    <p>AI safety isn't just an English-language, Western problem. We bring diverse global perspectives to every research project and safety evaluation.</p>
  </div>
  <div class="research-area">
    <h4>üîç Rigorous Research</h4>
    <p>Our methodologies are peer-reviewed, published, and battle-tested across hundreds of real-world AI systems and deployment scenarios.</p>
  </div>
</div>

<div class="two-column">
  <div class="research-area">
    <h4>‚ö° Immediate Impact</h4>
    <p>We don't just publish papers‚Äîwe create actionable solutions that organizations can implement today to make their AI systems safer.</p>
  </div>
  <div class="research-area">
    <h4>ü§ù Collaborative Approach</h4>
    <p>We work with academic institutions, industry partners, and government agencies to ensure our research addresses real-world safety challenges.</p>
  </div>
</div>

---

*Founded in 2023, Future Ethics has quickly become the global leader in multicultural AI safety research, with our methodologies now used by major AI labs and government agencies worldwide.*